WARNING: You are using pip version 21.0.1; however, version 22.3 is available.
You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.
/home/me66vepi/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/home/me66vepi/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1497: UserWarning: Calling .zero_grad() from a module created with nn.DataParallel() has no effect. The parameters are copied (in a differentiable manner) from the original module. This means they are not leaf nodes in autograd and so don't accumulate gradients. If you need gradients in your forward method, consider using autograd.grad instead.
  warnings.warn(
Traceback (most recent call last):
  File "/cluster/me66vepi/GAIN_working/GAIN/main.py", line 106, in <module>
    main()
  File "/cluster/me66vepi/GAIN_working/GAIN/main.py", line 76, in main
    train_gain_criterion(num_epoch,
  File "/cluster/me66vepi/GAIN_working/GAIN/src/gain/train_gain.py", line 42, in train_gain_criterion
    logits, logits_am, heatmap = gain(images, labels)
  File "/home/me66vepi/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/me66vepi/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 167, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/home/me66vepi/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py", line 177, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/home/me66vepi/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 86, in parallel_apply
    output.reraise()
  File "/home/me66vepi/.local/lib/python3.8/site-packages/torch/_utils.py", line 429, in reraise
    raise self.exc_type(msg)
AttributeError: Caught AttributeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/home/me66vepi/.local/lib/python3.8/site-packages/torch/nn/parallel/parallel_apply.py", line 61, in _worker
    output = module(*input, **kwargs)
  File "/home/me66vepi/.local/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/cluster/me66vepi/GAIN_working/GAIN/src/models/gain.py", line 138, in forward
    weights = F.adaptive_avg_pool2d(backward_features, 1)
  File "/home/me66vepi/.local/lib/python3.8/site-packages/torch/nn/functional.py", line 1036, in adaptive_avg_pool2d
    _output_size = _list_with_default(output_size, input.size())
AttributeError: 'NoneType' object has no attribute 'size'

